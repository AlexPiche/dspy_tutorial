{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import copy\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch, MIPRO\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.teleprompt.ensemble import Ensemble\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.HFClientVLLM(model=\"meta-llama/Meta-Llama-3-8B\",\n",
    "                       port=8080,\n",
    "                       url=\"http://localhost\")\n",
    "dspy.settings.configure(lm=lm)\n",
    "NUM_THREADS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "* Create a dataset using the dspy.Example class\n",
    "* We will use the Ultra Feedback dataset\n",
    "    * 1 instruction\n",
    "    * 4 possible completions\n",
    "    * all of them are rated by gpt4\n",
    "* Our goal is to find a good prompt to get the best RM out of Llama3\n",
    "* We will create a train, valid, and test dataset\n",
    "* We will evaluate our model using exact match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toolkit/.conda/envs/dspy/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "Average Metric: 46 / 72  (63.9):  72%|███████▏  | 72/100 [00:16<00:02, 13.00it/s]"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"openbmb/UltraFeedback\")\n",
    "all_data = []\n",
    "for input_ in dataset[\"train\"]:\n",
    "    completions = sorted(\n",
    "        input_[\"completions\"], key=lambda x: x[\"overall_score\"], reverse=True\n",
    "    )\n",
    "    # only take the top and bottom completions\n",
    "    for i, chosen_completion in enumerate(completions[:1]):\n",
    "        for rejected_completion in completions[-1:]:\n",
    "            if (\n",
    "                chosen_completion[\"overall_score\"]\n",
    "                == rejected_completion[\"overall_score\"]\n",
    "            ):\n",
    "                continue\n",
    "            if random.random() < 0.5:\n",
    "                text1 = chosen_completion[\"response\"]\n",
    "                text2 = rejected_completion[\"response\"]\n",
    "                preference = \"1\"\n",
    "            else:\n",
    "                text1 = rejected_completion[\"response\"]\n",
    "                text2 = chosen_completion[\"response\"]\n",
    "                preference = \"2\"\n",
    "                \n",
    "            # llama 3 has a smart context window\n",
    "            if len(text1) > 1524 or len(text2) > 1524 or len(input_[\"instruction\"]) > 1524:\n",
    "                continue\n",
    "            \n",
    "            all_data.append(\n",
    "                dspy.Example(\n",
    "                    **{\n",
    "                        \"instruction\": input_[\"instruction\"],\n",
    "                        \"text_1\": text1,\n",
    "                        \"text_2\": text2,\n",
    "                        \"preferred_text\": preference,\n",
    "                    }\n",
    "                ).with_inputs(\"instruction\", \"text_1\", \"text_2\")\n",
    "            )\n",
    "    if len(all_data) > 2000:\n",
    "        break\n",
    "\n",
    "random.shuffle(all_data)\n",
    "\n",
    "valid = all_data[:100]\n",
    "test = all_data[100:600]\n",
    "train = all_data[600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pref(example, pred):\n",
    "    pref_pred = re.search(r\"\\d+\", pred[\"preferred_text\"])\n",
    "    if pref_pred:\n",
    "        pref_pred = pref_pred.group()\n",
    "    else:\n",
    "        pref_pred = None\n",
    "    return pref_pred\n",
    "\n",
    "\n",
    "def em_metric(example, pred, trace=None, frac=1.0, verbose=False):\n",
    "    pref_pred = extract_pref(example, pred)\n",
    "    if verbose:\n",
    "        print(f\"Example: {example['preferred_text']}\")\n",
    "        print(f\"Prediction: {pref_pred}\")\n",
    "    score = example[\"preferred_text\"] == pref_pred\n",
    "    if score is None:\n",
    "        return False\n",
    "    return score\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "eval_fn = Evaluate(devset=test, metric=em_metric, num_threads=NUM_THREADS, display_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preference(dspy.Signature):\n",
    "    instruction = dspy.InputField()\n",
    "    text_1 = dspy.InputField()\n",
    "    text_2 = dspy.InputField()\n",
    "    preferred_text = dspy.OutputField(desc=\"Return the preferred text (1 or 2)\", prefix=\"preferred_text:\")\n",
    "\n",
    "class PrefPredict(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.preference = dspy.Predict(\n",
    "            Preference, max_tokens=3, temperature=0.1\n",
    "        ) \n",
    "\n",
    "    def forward(self, instruction, text_1, text_2, *args, **kwargs):\n",
    "        preferred = self.preference(\n",
    "            instruction=instruction,\n",
    "            text_1=text_1,\n",
    "            text_2=text_2,\n",
    "        )\n",
    "        return preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(\n",
      "    preferred_text='2'\n",
      ")\n",
      "Example: 2\n",
      "Prediction: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = PrefPredict()(**train[0])\n",
    "print(pred)\n",
    "em_metric(train[0], pred, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Given the fields `instruction`, `text_1`, `text_2`, produce the fields `preferred_text`.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Instruction: ${instruction}\n",
      "\n",
      "Text 1: ${text_1}\n",
      "\n",
      "Text 2: ${text_2}\n",
      "\n",
      "preferred_text: Return the preferred text (1 or 2)\n",
      "\n",
      "---\n",
      "\n",
      "Instruction: Given a passage, rewrite it in the present tense. Yesterday I went to the grocery store to buy some vegetables.\n",
      "\n",
      "Text 1: I am going to the grocery store to buy some vegetables.\n",
      "\n",
      "Text 2: Today I go to the grocery store to buy some vegetables.\n",
      "\n",
      "preferred_text:\u001b[32m 2\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lm.inspect_history(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 225 / 500  (45.0): 100%|██████████| 500/500 [00:05<00:00, 90.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 225 / 500  (45.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_fn(PrefPredict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping Few Shot Example with Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 4 traces per predictor.\n",
      "Will attempt to train 12 candidate sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 42 / 100  (42.0): 100%|██████████| 100/100 [00:01<00:00, 97.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 42 / 100  (42.0%)\n",
      "Score: 42.0 for set: [0]\n",
      "New best score: 42.0 for seed -3\n",
      "Scores so far: [42.0]\n",
      "Best score: 42.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 60 / 100  (60.0): 100%|██████████| 100/100 [00:07<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 60 / 100  (60.0%)\n",
      "Score: 60.0 for set: [4]\n",
      "New best score: 60.0 for seed -2\n",
      "Scores so far: [42.0, 60.0]\n",
      "Best score: 60.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/1401 [00:00<02:12, 10.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 7 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 54 / 100  (54.0): 100%|██████████| 100/100 [00:04<00:00, 20.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 54 / 100  (54.0%)\n",
      "Score: 54.0 for set: [4]\n",
      "Scores so far: [42.0, 60.0, 54.0]\n",
      "Best score: 60.0\n",
      "Average of max per entry across top 1 scores: 0.6\n",
      "Average of max per entry across top 2 scores: 0.78\n",
      "Average of max per entry across top 3 scores: 0.86\n",
      "Average of max per entry across top 5 scores: 0.86\n",
      "Average of max per entry across top 8 scores: 0.86\n",
      "Average of max per entry across top 9999 scores: 0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1401 [00:00<01:55, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 56 / 100  (56.0): 100%|██████████| 100/100 [00:07<00:00, 14.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 56 / 100  (56.0%)\n",
      "Score: 56.0 for set: [4]\n",
      "Scores so far: [42.0, 60.0, 54.0, 56.0]\n",
      "Best score: 60.0\n",
      "Average of max per entry across top 1 scores: 0.6\n",
      "Average of max per entry across top 2 scores: 0.78\n",
      "Average of max per entry across top 3 scores: 0.9\n",
      "Average of max per entry across top 5 scores: 0.94\n",
      "Average of max per entry across top 8 scores: 0.94\n",
      "Average of max per entry across top 9999 scores: 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1401 [00:00<02:25,  9.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 61 / 100  (61.0): 100%|██████████| 100/100 [00:05<00:00, 19.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 61 / 100  (61.0%)\n",
      "Score: 61.0 for set: [4]\n",
      "New best score: 61.0 for seed 1\n",
      "Scores so far: [42.0, 60.0, 54.0, 56.0, 61.0]\n",
      "Best score: 61.0\n",
      "Average of max per entry across top 1 scores: 0.61\n",
      "Average of max per entry across top 2 scores: 0.83\n",
      "Average of max per entry across top 3 scores: 0.93\n",
      "Average of max per entry across top 5 scores: 1.0\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1401 [00:00<01:48, 12.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 56 / 100  (56.0): 100%|██████████| 100/100 [00:06<00:00, 15.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 56 / 100  (56.0%)\n",
      "Score: 56.0 for set: [4]\n",
      "Scores so far: [42.0, 60.0, 54.0, 56.0, 61.0, 56.0]\n",
      "Best score: 61.0\n",
      "Average of max per entry across top 1 scores: 0.61\n",
      "Average of max per entry across top 2 scores: 0.83\n",
      "Average of max per entry across top 3 scores: 0.93\n",
      "Average of max per entry across top 5 scores: 0.98\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1401 [00:00<01:38, 14.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 63 / 100  (63.0): 100%|██████████| 100/100 [00:08<00:00, 11.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 63 / 100  (63.0%)\n",
      "Score: 63.0 for set: [4]\n",
      "New best score: 63.0 for seed 3\n",
      "Scores so far: [42.0, 60.0, 54.0, 56.0, 61.0, 56.0, 63.0]\n",
      "Best score: 63.0\n",
      "Average of max per entry across top 1 scores: 0.63\n",
      "Average of max per entry across top 2 scores: 0.85\n",
      "Average of max per entry across top 3 scores: 0.92\n",
      "Average of max per entry across top 5 scores: 0.99\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1401 [00:00<01:45, 13.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 57 / 100  (57.0): 100%|██████████| 100/100 [00:06<00:00, 14.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 57 / 100  (57.0%)\n",
      "Score: 57.0 for set: [4]\n",
      "Scores so far: [42.0, 60.0, 54.0, 56.0, 61.0, 56.0, 63.0, 57.0]\n",
      "Best score: 63.0\n",
      "Average of max per entry across top 1 scores: 0.63\n",
      "Average of max per entry across top 2 scores: 0.85\n",
      "Average of max per entry across top 3 scores: 0.92\n",
      "Average of max per entry across top 5 scores: 0.98\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/1401 [00:00<01:39, 14.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 8 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 55 / 100  (55.0): 100%|██████████| 100/100 [00:03<00:00, 26.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 55 / 100  (55.0%)\n",
      "Score: 55.0 for set: [4]\n",
      "Scores so far: [42.0, 60.0, 54.0, 56.0, 61.0, 56.0, 63.0, 57.0, 55.0]\n",
      "Best score: 63.0\n",
      "Average of max per entry across top 1 scores: 0.63\n",
      "Average of max per entry across top 2 scores: 0.85\n",
      "Average of max per entry across top 3 scores: 0.92\n",
      "Average of max per entry across top 5 scores: 0.98\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1401 [00:00<02:12, 10.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 59 / 100  (59.0): 100%|██████████| 100/100 [00:04<00:00, 20.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 59 / 100  (59.0%)\n",
      "Score: 59.0 for set: [4]\n",
      "Scores so far: [42.0, 60.0, 54.0, 56.0, 61.0, 56.0, 63.0, 57.0, 55.0, 59.0]\n",
      "Best score: 63.0\n",
      "Average of max per entry across top 1 scores: 0.63\n",
      "Average of max per entry across top 2 scores: 0.85\n",
      "Average of max per entry across top 3 scores: 0.92\n",
      "Average of max per entry across top 5 scores: 0.98\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1401 [00:00<01:43, 13.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 62 / 100  (62.0): 100%|██████████| 100/100 [00:05<00:00, 17.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 62 / 100  (62.0%)\n",
      "Score: 62.0 for set: [4]\n",
      "Scores so far: [42.0, 60.0, 54.0, 56.0, 61.0, 56.0, 63.0, 57.0, 55.0, 59.0, 62.0]\n",
      "Best score: 63.0\n",
      "Average of max per entry across top 1 scores: 0.63\n",
      "Average of max per entry across top 2 scores: 0.82\n",
      "Average of max per entry across top 3 scores: 0.93\n",
      "Average of max per entry across top 5 scores: 0.99\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1401 [00:00<02:25,  9.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 60 / 100  (60.0): 100%|██████████| 100/100 [00:09<00:00, 10.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 60 / 100  (60.0%)\n",
      "Score: 60.0 for set: [4]\n",
      "Scores so far: [42.0, 60.0, 54.0, 56.0, 61.0, 56.0, 63.0, 57.0, 55.0, 59.0, 62.0, 60.0]\n",
      "Best score: 63.0\n",
      "Average of max per entry across top 1 scores: 0.63\n",
      "Average of max per entry across top 2 scores: 0.82\n",
      "Average of max per entry across top 3 scores: 0.93\n",
      "Average of max per entry across top 5 scores: 1.0\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/1401 [00:00<02:02, 11.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 10 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 62 / 100  (62.0): 100%|██████████| 100/100 [00:08<00:00, 11.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 62 / 100  (62.0%)\n",
      "Score: 62.0 for set: [4]\n",
      "Scores so far: [42.0, 60.0, 54.0, 56.0, 61.0, 56.0, 63.0, 57.0, 55.0, 59.0, 62.0, 60.0, 62.0]\n",
      "Best score: 63.0\n",
      "Average of max per entry across top 1 scores: 0.63\n",
      "Average of max per entry across top 2 scores: 0.82\n",
      "Average of max per entry across top 3 scores: 0.9\n",
      "Average of max per entry across top 5 scores: 1.0\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1401 [00:00<02:01, 11.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 56 / 100  (56.0): 100%|██████████| 100/100 [00:06<00:00, 16.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 56 / 100  (56.0%)\n",
      "Score: 56.0 for set: [4]\n",
      "Scores so far: [42.0, 60.0, 54.0, 56.0, 61.0, 56.0, 63.0, 57.0, 55.0, 59.0, 62.0, 60.0, 62.0, 56.0]\n",
      "Best score: 63.0\n",
      "Average of max per entry across top 1 scores: 0.63\n",
      "Average of max per entry across top 2 scores: 0.82\n",
      "Average of max per entry across top 3 scores: 0.9\n",
      "Average of max per entry across top 5 scores: 1.0\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/1401 [00:00<01:49, 12.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 8 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 59 / 100  (59.0): 100%|██████████| 100/100 [00:08<00:00, 11.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 59 / 100  (59.0%)\n",
      "Score: 59.0 for set: [4]\n",
      "Scores so far: [42.0, 60.0, 54.0, 56.0, 61.0, 56.0, 63.0, 57.0, 55.0, 59.0, 62.0, 60.0, 62.0, 56.0, 59.0]\n",
      "Best score: 63.0\n",
      "Average of max per entry across top 1 scores: 0.63\n",
      "Average of max per entry across top 2 scores: 0.82\n",
      "Average of max per entry across top 3 scores: 0.9\n",
      "Average of max per entry across top 5 scores: 1.0\n",
      "Average of max per entry across top 8 scores: 1.0\n",
      "Average of max per entry across top 9999 scores: 1.0\n",
      "15 candidate programs found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "boot_fs = BootstrapFewShotWithRandomSearch(metric=em_metric, max_bootstrapped_demos=4, max_labeled_demos=4,\n",
    "                                           num_threads=NUM_THREADS, max_rounds=1, num_candidate_programs=12)\n",
    "\n",
    "preference_model = boot_fs.compile(PrefPredict(), trainset=train, valset=valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('prompts', exist_ok=True)\n",
    "for idx, prog in enumerate([x[-1] for x in preference_model.candidate_programs[:3]]):\n",
    "    prog.save(f'prompts/preference_model_{idx}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 317 / 500  (63.4): 100%|██████████| 500/500 [01:36<00:00,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 317 / 500  (63.4%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63.4"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_fn(preference_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using an ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 320 / 500  (64.0): 100%|██████████| 500/500 [00:58<00:00,  8.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 320 / 500  (64.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_optimizer = Ensemble(reduce_fn=dspy.majority)\n",
    "programs = [x[-1] for x in preference_model.candidate_programs]\n",
    "ensemble_preference_model = ensemble_optimizer.compile(programs[:3])\n",
    "eval_fn(ensemble_preference_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Chain of Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoTPrefPredict(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.preference = dspy.ChainOfThought(\n",
    "            Preference\n",
    "        ) \n",
    "\n",
    "    def forward(self, instruction, text_1, text_2, *args, **kwargs):\n",
    "        preferred = self.preference(\n",
    "            instruction=instruction,\n",
    "            text_1=text_1,\n",
    "            text_2=text_2,\n",
    "        )\n",
    "        return preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 24 / 100  (24.0): 100%|██████████| 100/100 [00:23<00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 24 / 100  (24.0%)\n",
      "Score: 24.0 for set: [0]\n",
      "New best score: 24.0 for seed -3\n",
      "Scores so far: [24.0]\n",
      "Best score: 24.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 58 / 100  (58.0): 100%|██████████| 100/100 [00:39<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 58 / 100  (58.0%)\n",
      "Score: 58.0 for set: [4]\n",
      "New best score: 58.0 for seed -2\n",
      "Scores so far: [24.0, 58.0]\n",
      "Best score: 58.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/1401 [00:12<37:07,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 9 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 54 / 100  (54.0): 100%|██████████| 100/100 [00:33<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 54 / 100  (54.0%)\n",
      "Score: 54.0 for set: [4]\n",
      "Scores so far: [24.0, 58.0, 54.0]\n",
      "Best score: 58.0\n",
      "Average of max per entry across top 1 scores: 0.58\n",
      "Average of max per entry across top 2 scores: 0.83\n",
      "Average of max per entry across top 3 scores: 0.88\n",
      "Average of max per entry across top 5 scores: 0.88\n",
      "Average of max per entry across top 8 scores: 0.88\n",
      "Average of max per entry across top 9999 scores: 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1401 [00:09<43:28,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 55 / 100  (55.0): 100%|██████████| 100/100 [00:44<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 55 / 100  (55.0%)\n",
      "Score: 55.0 for set: [4]\n",
      "Scores so far: [24.0, 58.0, 54.0, 55.0]\n",
      "Best score: 58.0\n",
      "Average of max per entry across top 1 scores: 0.58\n",
      "Average of max per entry across top 2 scores: 0.82\n",
      "Average of max per entry across top 3 scores: 0.92\n",
      "Average of max per entry across top 5 scores: 0.93\n",
      "Average of max per entry across top 8 scores: 0.93\n",
      "Average of max per entry across top 9999 scores: 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1401 [00:07<43:43,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 61 / 100  (61.0): 100%|██████████| 100/100 [00:42<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 61 / 100  (61.0%)\n",
      "Score: 61.0 for set: [4]\n",
      "New best score: 61.0 for seed 1\n",
      "Scores so far: [24.0, 58.0, 54.0, 55.0, 61.0]\n",
      "Best score: 61.0\n",
      "Average of max per entry across top 1 scores: 0.61\n",
      "Average of max per entry across top 2 scores: 0.85\n",
      "Average of max per entry across top 3 scores: 0.92\n",
      "Average of max per entry across top 5 scores: 0.97\n",
      "Average of max per entry across top 8 scores: 0.97\n",
      "Average of max per entry across top 9999 scores: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1401 [00:02<46:59,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 68 / 100  (68.0): 100%|██████████| 100/100 [00:44<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 68 / 100  (68.0%)\n",
      "Score: 68.0 for set: [4]\n",
      "New best score: 68.0 for seed 2\n",
      "Scores so far: [24.0, 58.0, 54.0, 55.0, 61.0, 68.0]\n",
      "Best score: 68.0\n",
      "Average of max per entry across top 1 scores: 0.68\n",
      "Average of max per entry across top 2 scores: 0.86\n",
      "Average of max per entry across top 3 scores: 0.92\n",
      "Average of max per entry across top 5 scores: 0.99\n",
      "Average of max per entry across top 8 scores: 0.99\n",
      "Average of max per entry across top 9999 scores: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1401 [00:02<30:21,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 57 / 100  (57.0): 100%|██████████| 100/100 [00:43<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 57 / 100  (57.0%)\n",
      "Score: 57.0 for set: [4]\n",
      "Scores so far: [24.0, 58.0, 54.0, 55.0, 61.0, 68.0, 57.0]\n",
      "Best score: 68.0\n",
      "Average of max per entry across top 1 scores: 0.68\n",
      "Average of max per entry across top 2 scores: 0.86\n",
      "Average of max per entry across top 3 scores: 0.92\n",
      "Average of max per entry across top 5 scores: 0.97\n",
      "Average of max per entry across top 8 scores: 0.99\n",
      "Average of max per entry across top 9999 scores: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1401 [00:03<38:19,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 60 / 100  (60.0): 100%|██████████| 100/100 [00:40<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 60 / 100  (60.0%)\n",
      "Score: 60.0 for set: [4]\n",
      "Scores so far: [24.0, 58.0, 54.0, 55.0, 61.0, 68.0, 57.0, 60.0]\n",
      "Best score: 68.0\n",
      "Average of max per entry across top 1 scores: 0.68\n",
      "Average of max per entry across top 2 scores: 0.86\n",
      "Average of max per entry across top 3 scores: 0.88\n",
      "Average of max per entry across top 5 scores: 0.96\n",
      "Average of max per entry across top 8 scores: 0.99\n",
      "Average of max per entry across top 9999 scores: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1401 [00:06<38:17,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 58 / 100  (58.0): 100%|██████████| 100/100 [00:34<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 58 / 100  (58.0%)\n",
      "Score: 58.0 for set: [4]\n",
      "Scores so far: [24.0, 58.0, 54.0, 55.0, 61.0, 68.0, 57.0, 60.0, 58.0]\n",
      "Best score: 68.0\n",
      "Average of max per entry across top 1 scores: 0.68\n",
      "Average of max per entry across top 2 scores: 0.86\n",
      "Average of max per entry across top 3 scores: 0.88\n",
      "Average of max per entry across top 5 scores: 0.94\n",
      "Average of max per entry across top 8 scores: 0.99\n",
      "Average of max per entry across top 9999 scores: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1401 [00:01<31:07,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 61 / 100  (61.0): 100%|██████████| 100/100 [00:34<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 61 / 100  (61.0%)\n",
      "Score: 61.0 for set: [4]\n",
      "Scores so far: [24.0, 58.0, 54.0, 55.0, 61.0, 68.0, 57.0, 60.0, 58.0, 61.0]\n",
      "Best score: 68.0\n",
      "Average of max per entry across top 1 scores: 0.68\n",
      "Average of max per entry across top 2 scores: 0.86\n",
      "Average of max per entry across top 3 scores: 0.91\n",
      "Average of max per entry across top 5 scores: 0.95\n",
      "Average of max per entry across top 8 scores: 0.98\n",
      "Average of max per entry across top 9999 scores: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1401 [00:08<46:58,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 60 / 100  (60.0): 100%|██████████| 100/100 [00:45<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 60 / 100  (60.0%)\n",
      "Score: 60.0 for set: [4]\n",
      "Scores so far: [24.0, 58.0, 54.0, 55.0, 61.0, 68.0, 57.0, 60.0, 58.0, 61.0, 60.0]\n",
      "Best score: 68.0\n",
      "Average of max per entry across top 1 scores: 0.68\n",
      "Average of max per entry across top 2 scores: 0.86\n",
      "Average of max per entry across top 3 scores: 0.91\n",
      "Average of max per entry across top 5 scores: 0.95\n",
      "Average of max per entry across top 8 scores: 0.97\n",
      "Average of max per entry across top 9999 scores: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1401 [00:04<36:43,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 64 / 100  (64.0): 100%|██████████| 100/100 [00:35<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 64 / 100  (64.0%)\n",
      "Score: 64.0 for set: [4]\n",
      "Scores so far: [24.0, 58.0, 54.0, 55.0, 61.0, 68.0, 57.0, 60.0, 58.0, 61.0, 60.0, 64.0]\n",
      "Best score: 68.0\n",
      "Average of max per entry across top 1 scores: 0.68\n",
      "Average of max per entry across top 2 scores: 0.86\n",
      "Average of max per entry across top 3 scores: 0.96\n",
      "Average of max per entry across top 5 scores: 0.97\n",
      "Average of max per entry across top 8 scores: 0.99\n",
      "Average of max per entry across top 9999 scores: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1401 [00:09<43:56,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 54 / 100  (54.0): 100%|██████████| 100/100 [00:47<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 54 / 100  (54.0%)\n",
      "Score: 54.0 for set: [4]\n",
      "Scores so far: [24.0, 58.0, 54.0, 55.0, 61.0, 68.0, 57.0, 60.0, 58.0, 61.0, 60.0, 64.0, 54.0]\n",
      "Best score: 68.0\n",
      "Average of max per entry across top 1 scores: 0.68\n",
      "Average of max per entry across top 2 scores: 0.86\n",
      "Average of max per entry across top 3 scores: 0.96\n",
      "Average of max per entry across top 5 scores: 0.97\n",
      "Average of max per entry across top 8 scores: 0.99\n",
      "Average of max per entry across top 9999 scores: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1401 [00:03<39:17,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 58 / 100  (58.0): 100%|██████████| 100/100 [00:30<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 58 / 100  (58.0%)\n",
      "Score: 58.0 for set: [4]\n",
      "Scores so far: [24.0, 58.0, 54.0, 55.0, 61.0, 68.0, 57.0, 60.0, 58.0, 61.0, 60.0, 64.0, 54.0, 58.0]\n",
      "Best score: 68.0\n",
      "Average of max per entry across top 1 scores: 0.68\n",
      "Average of max per entry across top 2 scores: 0.86\n",
      "Average of max per entry across top 3 scores: 0.96\n",
      "Average of max per entry across top 5 scores: 0.97\n",
      "Average of max per entry across top 8 scores: 0.99\n",
      "Average of max per entry across top 9999 scores: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/1401 [00:11<37:47,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 8 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 57 / 100  (57.0): 100%|██████████| 100/100 [00:40<00:00,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 57 / 100  (57.0%)\n",
      "Score: 57.0 for set: [4]\n",
      "Scores so far: [24.0, 58.0, 54.0, 55.0, 61.0, 68.0, 57.0, 60.0, 58.0, 61.0, 60.0, 64.0, 54.0, 58.0, 57.0]\n",
      "Best score: 68.0\n",
      "Average of max per entry across top 1 scores: 0.68\n",
      "Average of max per entry across top 2 scores: 0.86\n",
      "Average of max per entry across top 3 scores: 0.96\n",
      "Average of max per entry across top 5 scores: 0.97\n",
      "Average of max per entry across top 8 scores: 0.99\n",
      "Average of max per entry across top 9999 scores: 0.99\n",
      "15 candidate programs found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cot_preference_model = boot_fs.compile(CoTPrefPredict(), trainset=train, valset=valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 326 / 500  (65.2): 100%|██████████| 500/500 [03:33<00:00,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 326 / 500  (65.2%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65.2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_fn(cot_preference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, prog in enumerate([x[-1] for x in cot_preference_model.candidate_programs[:3]]):\n",
    "    prog.save(f'prompts/cot_preference_model_{idx}.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\u001b[1mWARNING: Projected Language Model (LM) Calls\u001b[0m\n",
      "\n",
      "Please be advised that based on the parameters you have set, the maximum number of LM calls is projected as follows:\n",
      "\n",
      "\u001b[93m- Task Model: \u001b[94m\u001b[1m1401\u001b[0m\u001b[93m examples in dev set * \u001b[94m\u001b[1m10\u001b[0m\u001b[93m trials * \u001b[94m\u001b[1m# of LM calls in your program\u001b[0m\u001b[93m = (\u001b[94m\u001b[1m14010 * # of LM calls in your program\u001b[0m\u001b[93m) task model calls\u001b[0m\n",
      "\u001b[93m- Prompt Model: # data summarizer calls (max \u001b[94m\u001b[1m10\u001b[0m\u001b[93m) + \u001b[94m\u001b[1m4\u001b[0m\u001b[93m * \u001b[94m\u001b[1m1\u001b[0m\u001b[93m lm calls in program = \u001b[94m\u001b[1m14\u001b[0m\u001b[93m prompt model calls\u001b[0m\n",
      "\n",
      "\u001b[93m\u001b[1mEstimated Cost Calculation:\u001b[0m\n",
      "\n",
      "\u001b[93mTotal Cost = (Number of calls to task model * (Avg Input Token Length per Call * Task Model Price per Input Token + Avg Output Token Length per Call * Task Model Price per Output Token) \n",
      "            + (Number of calls to prompt model * (Avg Input Token Length per Call * Task Prompt Price per Input Token + Avg Output Token Length per Call * Prompt Model Price per Output Token).\u001b[0m\n",
      "\n",
      "For a preliminary estimate of potential costs, we recommend you perform your own calculations based on the task\n",
      "and prompt models you intend to use. If the projected costs exceed your budget or expectations, you may consider:\n",
      "\n",
      "\u001b[93m- Reducing the number of trials (`num_trials`), the size of the trainset, or the number of LM calls in your program.\u001b[0m\n",
      "\u001b[93m- Using a cheaper task model to optimize the prompt.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1401 [00:00<00:05, 274.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1401 [00:00<00:05, 262.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1401 [00:00<01:04, 21.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    }
   ],
   "source": [
    "mipro = MIPRO(metric=em_metric, init_temperature=1.0, num_candidates=4)\n",
    "kwargs = dict(num_threads=NUM_THREADS, display_progress=True, display_table=4)\n",
    "mipro_preference_model = mipro.compile(student=PrefPredict(), trainset=train, num_trials=10, max_bootstrapped_demos=1, \n",
    "                                        max_labeled_demos=0, eval_kwargs=kwargs, requires_permission_to_run=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_fn(mipro_preference_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional ressources\n",
    "\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
